
An idea I've had for a little while related to programming that I call subset types
Though, there may also be union, intersection, complement, etc types

---

In this framework of thinking:
* Types are like collections of objects
* An variable conforming to a type is a member of that type

So all the usual set operations and predicates work on types:

* Union types:
$Z = X \cup Y$ means that if $z \in Z$ then $z \in X$

* Intersection types:
$Z = X \cap Y$ means that if $z \in Z$ then $z \in X$ and $z \in Y$

* Subset types:
$Z \sube Z$ means that if $z \in Z$ then $z \in X$

Since when class Z inherits from class X everything you can do with $x \in X$ can also be done with $x \in Z$, that makes $X \sube Z$ in this case
So inheritance is like a reverse subset type relation

* Complement types:
$Z \sube X^c$ means that if $z \in Z$ then $z \notin X$

And, a *tag type* is just a generic type (set) that doesn't necessarily have an explicit definition

---

Pseudocode (sort of D styled) example

```
UnitVector in Vector
Vector UnitVector normalize(Vector v) { return v/v.norm; }
```
Then you could do something like:
```
Real {1} norm(UnitVector v) { return 1; }
```

The compiler then can look at an expression like `Vector vec = Vector(...).normalize; f(vec.norm)` and replace it with just `f(1)`

Note: the `Vector UnitVector` and `Real {1}` type sequences in function definitions make an intersection type. In these cases since `UnitVector in Vector` this is equivalent to `UnitVector`, and the same for `Real`; but in a real world programming system it is likely advantageous to put the supertypes like this for clarity purposes

Even further, you could define something like:
```
Vector UnitVector cs(Real On(0, 2*pi) q) { return Vector(cos(q), sin(q)); }
```
Then later use a magical `auto` statement like
```
auto Vector Random UnitVector randomUnitVector()
```
Where `Random` is a tag type. This function `randomUnitVector` is automatically implemented by the compiler. How does it do this? It looks at defined functions and type relations (it could also look at other auto functions and define them) and composes them so their types are consistent

Note: there must be a definition somewhere to make a subset type for functions that take `Random` types, like `Random F(F)(Random r)`, so that the system knows it can spread `Random` types from input types to output types. Also, there must be a `Real Random random(...)` function somewhere. That's how the system can choose to generate random numbers via `random`, plug them into `cs` to get a vector, then apply `normalize` to that vector to make a `Random UnitVector`

Repurposing the theorem-proving and logic programming systems already in existence would allow us to make such a system *right now* I believe. This isn't even (very) futuristic technology, it's possible now

---

We could use this system to check for errors as well

This function
```
Real oneDividedBy(Real not({0}) x) { return 1/x; }
```
Divides 1 by the given x. But we obviously don't want x to be 0

Now if we do `Real x = 0; oneDividedBy(x)` the compiler automatically knows without any magic that this is an error (a type error!)

In the same vein:
```
Real log(Real Positive x) { ... }
Real Positive sqrt(Real Positive x) { ... }
...
```
Where `Positive in Real` is a tag subset type

How do we specify what `Positive` looks like?
```
Positive in Real
invariant Positive {
  assert(this > 0)
}
```
Of course, the invariant will generally only be checked at runtime, but the type `Positive` itself represents that the invariant is true, and its value is that it tags function outputs. And if there is a value specified at compile-time like `Positive x = 2`, the invariant can be checked at compile time

You may also do something like
```
Positive = Satisfies(this > 0)
```
Where `Satifies(...)` is a template subset type that does the exact same thing as the invariant definition above, but may be useful for specific cases like `Satisfied(7 < this < 15)` or something

Magic could extend this process. For example, if you had a `... foo(Satisfies(this > 0) x) {...}` definition, then later `Satisfies(this > 2) x = 4`; can we then do `foo(x)`? I think it would take rules built in to the solver for looking at predicates like `this > n` to determine if `this > 2` is a subset of `this > 0`, but I think it could be done in this case. I'm not sure whether that process could be extended to arbitrary predicates, though

Note: the more type relations the better, eg:
```
bool isIn(L1, R1, L2, R2)(Satisfies!(L1,R1)(L1 < this < R1) a, Satisfies!(L2,R2)(L2 < this < R2) b) {
  return L2 <= L1 && L1 <= R2 && L2 <= R1 && R1 <= R2
}
```
Can be used by the solver on the output of a function it thinks *may* be in a type, or when it can't find a solution using other types, so it extends its analysis outward

In general, the more paths from one type to another should allow the system to solve for more functions. There may even be some critical mass set of functions / type relations which allows the system to generate any possible function / type relation, assuming creating more type-relations is possible for the solver. This may be able to push the system past an AI-complete level solver

---

Another example

```
String HumanReadable AsciiOnlyString WhitespaceSplittable foo(...){...}
AsciiOnlyString in String
invariant AsciiOnlyString {
  foreach(x in this)
    assert(x in AsciiCharacter)
}
```

This type `AsciiOnlyString` could be simplified by defining:
```
ArrayContaining(L) in Array
invariant ArrayContaining(L) {
  foreach(x in this)
    assert(x in L)
}
AsciiOnlyString in ArrayContaining!AsciiCharacter
```
Assuming `String in Array`

We could also define a function type subset something like:
```
// function that only switches the places of elements, never sets them to new values
OnlyReordersArray in (Array F(F, Args...)(Args))
```
Then do something like:
```
String AsciiOnlyString F(OnlyReordersArray F)(AsciiOnlyString)
```

Constraining function types like this seems like it could be really useful. But there are usability issues with function subset types. eg: The above `F(OnlyReordersArray F)(AsciiOnlyString)` only works with a single parameter F and ignores `F(OnlyReordersArray F)(Integer, AsciiOnlyString)` for instance. It would take a syntax like `F(OnlyReordersArray, Args... containing AsciiOnlyString)(Args)` which seems cubersome and inelegant

---

I've also been playing around with the idea of *statement types*
eg: `if(x > 0) {f(x)} else {g(x)}` has an effect on the system state, and that effect has a type

Something like
```x = 5```
Could be called a `ChangedValueStatement` or `ValueChanged(x)` or something

Maybe `if(x > 0) {f(x)}` is like `HasEffect or NoEffect` depending on what f does. If f is `NoEffect` then the statement doesn't do anything and can be deleted entirely

---

Some other ideas related to this, and that push the concept of magical programming systems even further:

Magical `?-` and `?- maybe` operators:
```
x in String
?- maybe x == "hello" // true

x[0] = 't'
?- maybe x == "hello" // false

x in AlphanumericOnlyString
?- maybe x == "$hello" // false
```
Where `AlphanumericOnlyString` has a test which `?- maybe` uses that probably looks something like:
```
type AlphanumericOnlyString {
  bool op?-maybe(string value){
    foreach(element c in value)
      if(not (?- c in 'a'..'z' || ?- c in 'A'..'Z' || ...))
        return false
      return true
  }
  ...
}
```
Now `?- maybe` acts is a system-aware function which pulls in various test to determine truthiness / falsiness of its predicate

Async with `?-`:
```
async watch (?- x in AsciiOnlyString) {
  print("x in AsciiOnlyString")
}
```
This is async, meaning it's executed at indeterminate times. It will check its `?-` predicate whenever `x` changes and print "x in AsciiOnlyString" whenever the predicate is true
The system potentially uses statement types to determine when x changes, when to lock variables, etc

Async with type refinement:
```
x in All
async refine x
(x in String) <=> (y in AsciiOnlyString)
...
y = "hello"
?- x in String // true
```
The system does async refinement on its knowledge of x (type, value, etc) whenever that refinement is possible. It determines that refinement is possible when y is defined, so it checks if y's type is an `AsciiOnlyString`, and if so then x is a String

Arbitrary functions could be used as predicates:
```
trainset = loadFile(...)
bool function(...) classifier = trainANN(someNet, trainset, ...)
:- classifier(x)
```
This makes an incomprehensible function `classifier` which it asserts is true via the `:-` statement. Then whenever there is a `?- classifier(x)` or expression involving `classifier(x)` the system knows that is true. Since `?-` is necessarily a system-aware function, it may be able to work with `classifier` in other ways defined by the programmer or through type anaysis. For example: it could use a generative ANN which can produce inputs `x` to `classifier` that make `?- classifier(x)` true or false

Retrocausal assertions:
```
y in Integer, Positive
x = y + 1
:- x = 'X'
?- y == 'W' // true
```
Here `:-` forces x to be `'X'` and so the system can look back and determine what y would have been, then it can set y to `'W'`. Again `:-` is a system-aware function and can make use of any number of functions to determine what y is from x. In this case it probably has to use a built-in equation solver for equations like `x = f(y)`. In some cases it may not be possible for the solver to determine what y is from x. However, it may store all of its knowledge about y and refine its type over time, and so it may not be able to answer `?- y == 'W'` but may be able to answer `?- y in AsciiChar` definitively

This would truly be magic:
```
Vector position0 in Nearby(Vector(1,0), 0.2)
Vector velocity0 in Nearby(Vector(0,1), 0.2)
Real tn in Nearby(3, 2)
Vector position, velocity = simulateOrbit(position0, velocity0, tn)
:- position in Nearby(position0, 0.01)
:- velocity in Nearby(velocity0, 0.01)
```
Which makes the solver find a solution for `position0`, `velocity0`, and `tn` that makes the output of `simulateOrbit` (which simulates a particle orbiting around the origin like gravity (G=1,M=1)) match the input position and velocities within some small tolerance. The solver would almost certainly have to make use of predefined subsolvers to reverse `simulateOrbit`, and probably optimizers to find a solution within the given region

---

The idea of subset types, but set operations on types specifically, was born out of what-if style imagining what advanced computer systems would look like; specifically what programs that could extend themselves automatically might look like. This was very much like playing make believe actually

I believe this kind of thinking is important for a number of different reasons:

* It's fun!
Just like why kids do it... The fun is probably a primitive incentivizer so members of our species are encouraged to try new things and learn, thus increasing their odds of survival. Since we've sort of mesa-optimized the threat of the real world (mostly) out of our lives, the fun we get from imagining and playing doesn't actually have to have to correspond with any benefit, the fun is its own reward. But, there still may be the same potential long term benefits of learning and discovery

* It makes you effectively more creative
Through the babbling process and letting your filters down. To add another keyword: it increases you slack in the discovery process. This lets you round up a bunch of ideas then weed out the ones that aren't useful

* It drives you to work toward the future
By letting you see what things could be like, which makes a target for you to work for. Without those targets its extremely easy to become too myopically focused on the trees that you miss the forest
Not everyone is future-seeking, of course, but for the people who want a future that is more satisfying, entertaining, happy, content, harmonious, exciting, engaging, interesting, etc, they must have a vision of what that future looks like. And there aren't necessarily any limits on what those visions of the future are like, just that they continue to push you toward improving the circumstances of you and your community (presumably; I suppose you could also work to make your circumstances worse, but why would you???)

Once you have a vision, then you try to find / solve for ways toward that actually realizing that vision. In this case I imagine an automatic programming system, so I imagine what that would look like, I imagine what is possible and what we have that can be composed together to make a system more like that, etc. Once some individual features of a solution are composed together consistently[^1], those can be effectively ignored and the solution can continue to be grown toward one that will solve the problem.

Its worth pointing out that the principle strategy in playing make believe like this is just *finding something that works*. Which may be an integral strategy in intelligence itself. In humans we seem to have really incredibly powerful built-in generative solvers, so this kind of thing (finding solutions to something, even if they're ridiculous) generally comes quite easily

---

[^1]: It seems likely that the representations your brain uses for each part of a solution to a problem have 'types' (probably most like tag types; with internal-semantics) which must agree to be properly composed together. What constitutes agreement here?












