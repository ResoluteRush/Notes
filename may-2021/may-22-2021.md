I experimented with set min random loss again. Just kind of playing around

---

On multiple MNIST AEs

`dat` is just the image portion of elements of MNIST (5000 elements). `sl` is the size of an mnist image element $28^2$

Trained `basenet` network using:
```mathematica
dl = 10;
basenet = NetTrain[NetChain[{
     ApproximatorLayer[sl, dl],
     ApproximatorLayer[sl, sl],
     ElementwiseLayer["Sigmoid"]
     }], <|"Input" -> dat, "Output" -> dat|>, TimeGoal -> 15*60, 
   LossFunction -> {"Output" -> MeanSquaredLossLayer[]}];
```

Where
```mathematica
ApproximatorLayer[size_, out_, act_ : Tanh] := 
 NetChain[{LinearLayer[size], ElementwiseLayer[act], 
   LinearLayer[If[TrueQ[out == 0], "Real", out]]}]
```

Checked its output with 
```mathematica
Table[Column[{tmp1 = RandomChoice@dat;
   ShowTiny@decf@tmp1,
   ShowTiny@decf@basenet@tmp1}], {4}]
```

Where `ShowTiny = Show[#, ImageSize -> Tiny]&` and `decf = Image@Partition[#,28]&`. It converged good

Then made a `ListLayer` net with:
```mathematica
pnum = 3;
net = NetChain[{
    ListLayer[Table[basenet, pnum]]
    }, "Input" -> sl, "Output" -> {pnum, sl}];
```

`NetSize = Information[#, "ArraysTotalElementCount"]&` gave `3 742 062` count

Trained with
```mathematica
net = Check[
  NetTrain[net, <|"Input" -> dat, "Output" -> dat|>, 
   TimeGoal -> 15*60, LearningRate -> 10^-2, 
   LossFunction -> {"Output" -> 
      SetMinRandomLoss[MeanSquaredLossLayer[], 10^0 (*Then switch to 10^-1 and train again*), 
       "Target" -> sl]}], net]
```

Where `SetMinRandomLoss` is
```mathematica
SetMinRandomLoss = 
 With[{layer = #1, mag = #2}, 
   If[mag == 0, SetMinLoss[layer], 
    NetGraph[{"rep" -> ReplicateLayer[Automatic], 
      "mapt" -> NetMapThreadOperator[layer], 
      "rand" -> 
       NetChain[{RandomArrayLayer[NormalDistribution[0, mag]], 
         ElementwiseLayer[Exp]}], "*rand" -> ThreadingLayer[Times], 
      "min" -> AggregationLayer[Min, 1]}, {NetPort["Input"] -> 
       "mapt" -> "*rand" -> "min", "rand" -> "*rand", 
      NetPort["Target"] -> 
       "rep" -> NetPort["mapt", "Target"]}, ##3]]] &
```

On the $10^0$ session, the loss actually increases initially from the `basenet`'s loss. Only trained long enough to see what the loss does. Gave subtly different outputs even after 2 minutes or whatever of training. This indicates viability for producing variant outputs

The idea behind __set min loss__: you produce a bunch of similar outputs, find the regular loss of each output individually wrt the target, then take the min of those. So the network is encourages to improve the smallest loss. The idea being that this will encourage the net to make some of its variant subnetworks specialize to particular outputs as a way to effectively tiebreak possible outputs

The idea behind __set min random loss__: set min loss tends to just cause one variant subnet to improve indefinitely as its loss will generally be closer to every training set output than the other variant subnet. So you add normal noise in log space (multiply by log normal noise) for each of the variant subnet's losses and then you take the min. This causes there to be a chance that the system trains other variant subnets other than the absolute minimum loss one

---

I also tried a few different kinds of set min loss variants with learned target-informed weighting so the individual variant subnets could choose which training set elements to train on. But none of the nets I tried really did anything. I'll have to think about it again and come back to it

