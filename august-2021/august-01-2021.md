
I've been sick the last few weeks, and haven't really made any progress

---

Today I discovered that if you thread a weight tensor over the elementwise losses between your output and target tensors in an ANN before you take the sum of those loss elements, that you can essentially control the uncertainy of each element in the loss

eg: Here is the mean square loss between the output elements $y_i$ and target elements $t_i$:

$L = \sum_i m_i\, (y_i - t_i)^2$

The $m_i$ are the weight elements. More certain loss elements get a weight thats larger than less certain elements. Extremely certain elements could be given a very high weight, and they would be prioritized much more extremely than other elements. Very uncertain elements could be given a weight close to zero, and they would be essentially ignored

To map $[0,1]$ to $[0, \infty)$ I think a good function is $f(x) = \frac{x}{1-x+1/\epsilon}$ which is equivalent to $f(x) = \exp(S^{-1}(x))$ (plus a factor $1/\epsilon$ to avoid infinities) where $S(x)$ is the logistic sigmoid function. Using this we can use weights on $[0,1]$, with 0 mapping to 0, and 1 mapping to $\epsilon$ which is some large number

Also, you can normalize the loss which may help convergence and makes loss graphs in time more consistent:

$L = \frac{\sum_i m_i\, v_i}{\sum_i m_i}$

Which makes this into a weighted sum, and means this is a weighted mean square loss. Something similar could probably be used for cross entropy loss, etc

---

How might this be useful?

If you are training a network containing a binary classifier somewhere inside it, and you know that a single element of the classifier represents some quality, then you can provide the training process with weak information by adding a weighted mean square term to the loss. More concretely: if your internal classifier $C = (a, b)$ has an output that isn't known in principle during training because its input and outputs are both generated during training, and you know $a$ in $C$ but don't know $b$ (maybe because of some other network, or something), then you can use weighted mean square loss to encourage *just* the $a$ element in $C$ to go to the correct value
