
I noticed a similarity between the primitive forms of indexing, and semantic triple relationships

---

Indexing looks like `f(dog, tail) = 'bushy tail'`. Of course, in a realworld system all of these words would probably be vectors or higher order tensors in some space, or integer symbols

Indexing takes an object, an index object, and produce another object. It is exactly equivalent to regular ol' indexing in vectors $x_i = y$, and indexing of structures `x.i = y`. Though, there is the potential for indexing to be done with a continuous function as in the case of ANNs, and it looks like $f(x,i) = y$

Semantic triples / semantic relationships look like `f(dog, has) = tail`. Again these would probably be encoded as structures of numbers in a real world system

Semantic relationships a la semantic networks, like this, essentially are just data that associate three items in the same structure, but practically they take an object, a relationship type, and produce another object. Just like indexing, these could potentially be continuous functions $f(x,r) = y$

---

Obviously, these are the exact same form: $f(x, i) = y$, and $f(x, r) = y$. So in terms of a system made from two-input, one-output FCEs, these two functions are performed by the exact same mechanisms

Another interesting thing about this: you automatically get both indexing and semantic relationships like this when you can do sequence prediction with two element sequence inputs. If you have language and specifically the sequence prediction part of language, for instance, you automatically have the infrastructure for indexing and semantic relationships. This is like a text prompt: "complete the sentence: dog has ____". You could obviously always ask questions of the system, and it would answer them, and the answer would be a human-semantics translation of the data they contain

Also, notice that you can always do a 2-gram to 1-gram / symbol reduction, so any sequence of N elements can be reduced to a sequence of 2 elements in principle. This is related to the idea that any Nary function can be reduced to chains of binary functions. Unlike the examples above with symbols / representations like 'dog', 'tail', and 'has'; representations in a real system probably don't have clean one-word translations into human semantics, so you might end up with something mostly equivalent to `f('my neighbors dog', 'whats at the tail location on a dog') = 'a bushy tail with a white spot'`. Here, all of the representations are probably compositions of more elementary representations like 'dog', 'tail', etc; which implies that the principle behind the N-gram to 1-gram thing is probably happening here too


