
I had the realization that the purpose of any top-level module in a system of modules must have human-semantics

---

What this means:

A system here is like a network of modules[^1], with each edge / connection between any two modules having some relationships to the interfaces of both the input and output modules. ie: the 'from' / upstream module is contributing some output data via an output interface to the edge, and the edge is going into some part of the 'to' / downstream module's interfaces

Each module performs some function for the system, these are their *purpose*

Individual modules may have a system of child modules, in which case the parent provides some io interfaces for the child modules' environment. A *top-level* module is a module which isn't the child of another module

Data semantics is harder to describe. In this case human-semantics is a data semantics which a given human can understand in general (given they can understand the language it's written in, context, etc). A system's internal data semantics is generally scrambled (from a human-semantics perspective) and highly correlated to the system's functionality. In general data-semantics comes from correlation with a given system's functionality. Examples: a standard autoencoder's output (its 'hidden layer' / encoder layer) has internal and non-human semantics; a encryption algorithm produces data with internal semantics; and a lossless compression algorithm also produces data with internal semantics. And *external* semantics is data that appears on input devices (cameras, microphones, etc), and is generally comprehensible to humans when given enough context data. In general, if a system doesn't *have* to have human-semantics, then that allows the developer to essentially ignore the semantics completely, because the correlation[^2] process will find a data semantics. Furthermore, if the developer is only thinking of functions in terms of human comprehensible outputs, then they are limited to only FCEs[^3] between data with external semantics, and FCEs to human semantics outputs (which generally must be specifically specified by a human). Without UCEs[^3], you cannot reduce, isolate, or remove dimensionalities[^4], and all of these generally require internal semantics. Conceivably you can correlate individual subnetworks which each individually have human-semantic outputs, but this is limiting in its own way as well

So the statement 'the purpose of any top-level module in a system must have human-semantics' means that the indended function of the components making up the highest level of a system must be understandable by the developer. If this is true then it's really important. The major consequence of this is that you can piece the system together at the highest 

Note: this doesn't mean that the data-semantics a module consumes or produces has human-semantics, just that the indended behavior of that module performs in the system is comprehensible by the developer. A module that doesn't have human semantic purpose would be one that does produce an apparently useful effect, and allows the system to function the way it's intended to function, but whose effect isn't something comprehensible by a person. eg: A module that transforms some data into data of the same dimension. But *why* would the system even include a module like that? Because the developer saw that including the module makes the system work, despite not knowing what it's doing. Otherwise, a developer would only include the module if the module had some comprehensible reason for its existence (it has a human semantic purpose)

A stronger version of this is like 'the purpose of *any* module must have human-semantics'. This would include every child module. As a consequence, this probably makes the entire system's topology human comprehensible, despite possibly being hidden behind the complexity and size of the topology

---

I also, today, made a proof of concept dimensionality partitioner AE. It really works, though requires you to provide examples of elements from the same equivalence classes, ie: $f\ x_i = f\ x_j$ when $x_k \in X$ is a given class which you want to 

---

[^1]: Here, a module is like a black box with a relatively simple interface which other modules interact with it through. Presumably all modules have inputs (their interface), and produce outputs to other modules, or expose some state on an output interface which other modules can pull data from

[^2]: A function that can be 'correlated' to some data $T$ looks like $y = f(x; P(T))$ where $x, y$ are its inputs and outputs respectively, and $P$ is some value produced from the data. This means $f$ is correlated to $T$ if there is some parameter $P(T)$ which it can take that is specific to $T$. Usually $P(T)$ makes $y = f(x;P(T))$ iff $(x,y) \in T$, ie: $P(T)$ makes $f$ match some given input-output values appearing in $T$

[^3]: FCEs are Fully Correlated Edges, which means a function like $f(x; P(X,Y)) = y$ where $X, Y$ are the input and output sets respectively; ie: $f$ is correlated with both the input and output sets, and bot hare required for correlation. UCEs are UnCorrelated Edges, which are a bit of a misnomer that means a function like $f(x; P(X)) = y$ where $X$ is the input set; ie: $f$ is correlated with the input set only. Also under the term UCE are function like $f(x; P(Y)) = y$ which means function only correlated with the *output* set. An AE (autoencoder), for example, is trained in a network like $d(e(x)) = x$ where the output of $e$ is the output of the UCE. This is obviously only dependent on the input set. The output-dependence-only analog to an autoencoder is something you might call an autodecoder which is trained in a network like $e(d(y)) = y$

[^4]: Some data's dimensionality is essentially how many dimensions its information is embedded in, and doesn't have to equal its dimension. Informally, a dimensionality of data is some dimension of the information in the data. eg: The 'happiness', 'sadness', etc of a face in an image are each dimensionalities of the image (or images from the semantic extension of that image). Dimensionality reduction (dred) reduces the dimensionality of the data (preferably only removing the least descriptive dimensionalities), usually while also decreasing the data's dimension. AEs and distance-matching encoders are probably the simplest ways to do dimensionality reduction. Dimensionality isolation is the extraction of sets of dimensionalities, or partitioning some data's dimensionalities. ~~I'm currently unsure how to do dimensionality isolation, as I can't get anything to work~~ See above. Dimensionality removal is specifically choosing which dimensionalities to remove. This is probably possible through invariance $f\ r\ x = f\ x$ and equality $f\ x = f\ y$ relationship constraints. You can also apparently do dimensionality reduction using the equation $\nabla_ v f = 0$ where $v$ is some correlated constant vector or vector valued function of the input, this forces $f$ to be independent of some coordinate in some coordinate transformation

