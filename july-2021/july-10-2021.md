
I've been thinking about: if you have a set-correlated function like $f(x, T(X,Y)) = y$, then that function has an intended functionality (similar to what I described a few days ago) which makes what you might call a intentional interface, and that intentional interface has human semantics at some level; so a system of functions like this can be worked on as if they each are just their intentional interfaces, without thinking about how exactly they're implemented. That's just a convoluted way to say that every independent functional module in a system of such models can be thought of as doing a certain job that the developer of the system understands, despite *how* that module achieves that functionality being completely incomprehensible (because it has internal semantics)

eg: If I chain two dredding functions $f, g$ into a system $h = g\ f$, I know that the intentional interface of both of those functions is specifically like 'compresses the input by retaining only its most significant dimensionalities' or something like that, and so I know that the composition $h$ must also have this intentional interface. Despite the output of any of these functions being completely incomprehensible. In the case of ANNs, for instance, their outputs would look pretty random and would take effort to establish meaning for. But the purpose or intention -- the intentional semantics -- of the output has human semantics because we know its a dimensionality-reduced version of the original input

So just like the other day, how is this useful? Because you can build the system with human-semantics, then have the system correlate itself to its given io sets

Practically speaking, what are the intentional interfaces that would be useful? First, one that does dimensionality reduction (dimension reduction is probably optional, but likely). Another intentional interface could be forcing invariances: $e(f(x)) = e(x)$ and equivalently, equivalences: $e(x) = e(y)$. Distance matching $d(x, y) = d'(e(x), e(y))$ is another. Each of these is a different intentional interface, but exactly how those functions look when correlated isn't important, only that they perform some purpose. eg: A dredding function compresses the information from the input, invariant functions and functions with equivalences also compress information and generalize, distance matching functions generally compress information too (because they likely produce a lower dimension output) and they may be usable to make metrics for measuring how analogical things are

If you can establish composition relationships between different intional interfaces, then you probably can guess which networks actually make sense. For instance, a chain of dredding AEs is itself just a dredding AE, so one is essentially good enough. There are some effects of depth on how these functions correlate itc, but for first order approximations two AEs just make another AE. Furthermore, a network composed of an AE that branches into two more AEs has two outputs, and those outputs wrt the input have essentially equivalent intentional interfaces, so they can be combined. This means any *tree* of AEs is equivalent to a single AE. Note itc: if each output is correlated with downstream sets then this relationships breaks down; eg: if the subnet between the input and output 1 is required to produce set X and the subnet between the input and output 2 is required to produce set Y, then the information that each AE in those subnets retains is not just the most important information, but som information dependent on sets X and Y. Anyway, this is an extremely important result (when / if true) because it means we can replace all trees of AEs with a single AE

The intentional interfaces above are *UCEs*, and there are also *FCE* intentional interfaces, which generally look like: map x to y because they co-occur. Sequence prediction / prediction in time looks like that: map the state now to a state just in the future. There are also dimensionality expansion functions which add information back in. These are (probably generally) FCEs as well because they are dependent on both their inputs and their outputs

It *seems* like, you end up with just three types of intentional operations: dimensionality reducing functions, dimensionality expansion functions, and arbitrary maps between sets. So a hypothetical AGI using something like this would at the highest level look like a dimensionality reducing function from input device data to internal state, a map from internal state to dimensionality-reduced output state, and a map from that dredded output state to an action on an output device. There obviously must be much more to it to get the system to actually work, but at the highest level this is my broadest approximation what it looks like

This makes sense, too: you first remove the information you don't need from input with dredding operations, then you use maps built from experience to bring that internal state to a good action state, then you contextualize the encoded action state by adding information back in and make it viable for output devices. The question that remains is how to correlate your subnets on subsets of the universe of their dependent sets such that those functions are accurate (according to whatever arbitrary accuracy function) on those depedent set universes as well. ie: that the correlation generalizes to a larger set of io values
