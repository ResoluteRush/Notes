
I was trying various methods of using ANNs to cluster data and I hit on a method that works pretty well (at least as well as k-means afaict)

It looks like an AE with the encoder a generic approximator[^1] into a softmax layer, and the decoder a regular weight only linear layer. Then you can run the softmax in the encoder into a `1-max` layer to encourage unit-vectorization of that vector

It seems to like getting stuck on plateaus for awhile until it gets unstuck and rapidly improves before getting stuck again. This is probably because the unit vector `1-max` loss term resists free movement of class regions. I clipped the `1-max` loss term to a low value (like 0.2 or smaller) and that helped

Another issue is these tend to want to use *about half* of their softmaxxed vector's elements as ifaict for increasing the accuracy of the decoder. It's effectively like the class vector / softmaxxed vector has some dimensions which make a class which is more or less unit-vectorized, and some dimensions which have low values and act like a regular AEs encoded space. To put it another way: the class vector resists unit-vectorization because that would degrade the decoder's accuracy, and most of the decoder's accuracy comes from the values which aren't zero or one in the class vectors

Pretraining with k-means classes then training like regular forces the system to make use of all class vector elements, unit-vectorizes the class vectors, and the resulting output points don't move very much from the k-means class means. Whatever that means


---

[^1]: An approximator is a linear layer, activation function, linear layer network. The shallowest universal approximator