
You obviously need to have classified your universe of input elements into sets to be able to train a representer function with an equivalence relationship. But the system has to have representations to begin with or else it literally can't conceive of those objects enough to classify them. This indicates a dependence between classification and representation that is highly cyclical. This makes me think of GANs and how they function

Some potential explanations to the contrary of / alternatives to / relatives of this cycle; these are no-intelligence-needed approaches:

* It's always necessary to have more data to separate class distributions and makes it easy to cluster them. Does this mean you necessarily always need super high dimensional inputs synthesized from multiple disparate sources?
* Bias for certain forms of input more than others. eg: like what CNNs do
* You always only use invariance relationships with fixed invariance functions. ie: $f\ r\ x = f\ x$ where $r$ is given by the developer

One thing to keep in mind is that classification for certain things (in a brain) is a time-dependent process, where priors from disparate and miscellaneous sources across the system are combined as evidence for the new classification. This indicates an intelligence-needed process is used (again, in brains) to classify high level concepts.

