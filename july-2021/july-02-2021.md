
I was playing around with probabilities. This is related to probability network compression

---

If you keep applying the rule $P(x\lor y) = P(x) + P(y) - P(x \land y)$ to a larger disjunction you get $P\left(\bigvee_{x \in X} x\right) = \sum_{\chi \in \mathscr{P}(X)} (-1)^{\left|\chi\right|+1} P\left(\bigwedge_{x\in \chi} x\right)$ where $\mathscr{P}(X)$ is the power set of $X$

Then if we create values $y_i$ which have the property that $P\left(y_i \, | \, \bigvee_{x\in \chi} x \right) = 1$ where for each possible $\chi$ there is one $y_i$

Our disjunction probability is now like $P\left(\bigvee_{x\in X} x\right) = \sum_i \alpha_i P(y_i)$ where the $\alpha_i \in \{-1,1\}$. This incidentally (probably?) looks like a ANN with these layers in order: `approximator`[^1], `softmax`, `linear layer`

You can also, of course, do something similar with partitions: $P\left(\bigvee_{x\in X} x\right) = \sum_i P(z_i)$ where the $z_i$ partition $\bigvee_{x\in X} x$; ie: $\bigvee_{x\in X} x = \bigvee_i z_i$ and $z_i \land z_j = \emptyset$. Decomposing all of the $x\in X$ into their individual intersections is equivalent: $P\left(\bigvee_{x\in X} x\right) =  \sum_i P\left(\bigwedge_{x\in X}\begin{cases} x & s_{x\,i} = 1 \\ \lnot\, x  & s_{x\, i} = 0\end{cases}\right)$ where for each $x$ there is $s_{x\, i} = 1$ for some $i$ and $s_{x\, i} = 0$ for some $i$; this essentially means each possible intersection between all the $x$es are summed over

Note: for conjunctive / joint probability, you just swap your $\bigvee$s and $\bigwedge$s

---

What is the significance of this? It seems related to compressing probability networks, though I don't know how to do this yet

When you collect regular ol' frequency data to detect conditional probabilistic correlations between elements in your data, that information piles up quickly. It appears that this situation has $O(2^n)$ memory complexity where $n$ is the 'size' of your conjunctions: number of elements in them. ie: $P(z\,|\, x\land y)$ has size 2 and there are $2^2 - 1 = 3$ max possibilities in this system: $P(z\,|\,x\land y), P(x\,|\,z\land y), P(y\,|\,x\land z)$

I know you can compress regular networks like $\chi \subseteq X \overset{\forall\ \forall}{\longrightarrow} \zeta \subseteq Y$[^2]by adding another node between the two sets: $\chi \subseteq X \overset{\forall\ \exists}{\longrightarrow} \lambda \overset{\exists\ \forall}{\longrightarrow} \zeta \subseteq Y$ where $\lambda$ is a single element (or single element set). This reduces the edge count complexity from $O(n\, m)$ to $O(n + m)$, which is a logarithmic improvement (really good!). Theres some kind of business with edge labelling that's hard to express: the edges leading into the new single element node are 'is' edges, which means the single element node is equivalent to each element in the set node

It seems like there is an analog to this with probabilitic networks. Consider, if you have a network with $P(y \,|\, x)$ for each $x\in X,y\in Y$ you have $O(n\, m)$ conditional probability complexity, because for each $n$ of the $x$es you have $m$ of the $y$s. You can create a new value $\lambda$ such that $P(y\land x\land \lambda) = P(y\,|\, \lambda \land x)\, P(\lambda \land x) = P(y\,|\, \lambda \land x)\, P(\lambda \,|\, x)\, P(x)$ and assuming that $P(y\,|\,\lambda \land x) = P(y\,|\,\lambda)$ we can turn this system into a network with conditional probabilities $P(y\,|\, x) = P(y\,|\,\lambda)\, P(\lambda\,|\, x)$. So we can ignore the $P(y\,|\,x)$ and so in terms of edge (conditional probability) counts we have $n$ of the $P(\lambda\,|\,x)$ and $m$ of the $P(y\,|\,\lambda)$ so we have $O(n+m)$

In the above paragraph's compressed network, what is $\lambda$, then? Well, it seems to me that $P(\lambda) = P\left(\bigvee_x x\right)$ is a reasonable interpretation (though not necessarily universally true). This interpretation / assumption is why I saw graph compression in the last section's stuff

Notice that because this process goes from $n\, m$ edges to $n + m$ edges, it is inherently lossy

---

[^1]: An approximator looks like `linear layer`, `nonlinear activation function`, `linear layer` in that order. It's the most shallow universal approximator. Because it's a universal approximator, you can just slap one of these bad boys wherever you want a generic function, and it will learn a function that works

[^2]:This notation means for each element in the subset on the left there is an edge that goes to each element in the subset on the right
